#!usr/bin/env python
#-*- coding:utf-8 -*-
"""
@author:dmsoft
@file: paper_practice.py
@time: 2020/03/03
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd
from sklearn.pipeline import Pipeline
import time

import matplotlib

# 解决中文字体显示异常
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['font.family']='sans-serif'
# 解决负号’-‘显示为方块的问题
matplotlib.rcParams['axes.unicode_minus'] = False

class linera_pred(object):
    """
    线性回归
    """

    def __init__(self):
        self.x = 2 * np.random.rand(100, 1)  # rand 随机(0~1) randn 正态分布随机数(-1~1)
        self.y = 4 + 3 * self.x + np.random.randn(100, 1)  # 4*x0 + 3*x1 +random
        self.x_b = np.concatenate((np.ones((100, 1)), self.x), axis=1)  # add x0=1 to each instance

    def show_res(self,theta,label="predict line"):
        x_new = np.array([[0], [2]])
        x_new_b = np.c_[np.ones((2, 1)), x_new]
        y_predict = x_new_b.dot(theta)
        plt.plot(x_new, y_predict, 'r-',label=label)
        plt.axis([0, 2, 0, 15])
        plt.legend( loc="upper left")
        plt.show()

    def linear_reg(self):
        """线性回归，直接公式法  theata = (xT.x)-1 . xT . y """
        theta_best = np.linalg.inv(self.x_b.T.dot(self.x_b)).dot(self.x_b.T).dot(self.y)
        plt.scatter(self.x, self.y)
        self.show_res(theta_best)

    def linear_reg_sklearn(self):
        """采用sklearn  LinearRegression模型，直接求特征值法
        偏置项:lin_reg.intercept_ , 特征权重：lin_reg.coef_"""
        from sklearn.linear_model import LinearRegression
        lin_reg = LinearRegression()
        lin_reg.fit(self.x,self.y)

        theta = [lin_reg.intercept_, lin_reg.coef_]
        plt.scatter(self.x, self.y)
        self.show_res(theta)

    def lin_grad(self):
        """批量梯度下降法
        gradients = 2/m * self.x_b.T.dot(self.x_b.dot(theta) -self.y)
        批量梯度下降 超参数：学习率、迭代次数
        学习率：步长 过高->发散  过低->耗时
        对局部最小值曲线难以得到全局最小值"""
        eta = 0.1 # learning rate
        n_iterations = 1000
        m = 100
        theta = np.random.randn(2,1)
        plt.scatter(self.x, self.y)
        for iteration in range(n_iterations):
            gradients = 2/m * self.x_b.T.dot(self.x_b.dot(theta) -self.y)
            theta = theta - eta*gradients
            MSE = 0
            for i in range(self.y.shape[0]):
                MSE += 1/m *(((theta.T.dot(self.x_b[i]) - self.y[i])[0])**2)

            if iteration%20 == 0:
                print(MSE,theta)
                self.show_res(theta)
                plt.pause(0.01)

    def rand_grad(self,batch=1):
        """
         随机梯度下降法 超参数:迭代次数、学习率（t0,t1)
         gradients = 2/batch*xi.T.dot(xi.dot(theta)-yi)
         advantage: 有助于跳出局部最小值
         shortage: 永远找不出最小值，可逐步降低学习率使结果尽量靠近
        :param batch: 每次随机梯度采用数据个数
        """

        n_epochs = 50
        t0,t1 = 5,50 # learning schedule hyper-parameter
        m = 100
        theta = np.random.randn(2,1) # random initialization

        plt.scatter(self.x,self.y)
        for epoch in range(n_epochs):
            for i in range(m):
                random_index = np.random.randint(m)
                # 随机梯度下降
                if batch == 1 or not isinstance(batch,int):
                    xi = self.x_b[random_index:random_index+1]  # xi.shape: [1,2]
                    yi = self.y[random_index:random_index+1]
                    gradients = 2 * xi.T.dot(xi.dot(theta) - yi)

                # 小批量梯度随机下降
                else :
                    xi = self.x_b[random_index:random_index + batch]  # xi.shape: [1,2]
                    yi = self.y[random_index:random_index + batch]
                    gradients = 2/batch*xi.T.dot(xi.dot(theta)-yi)

                eta = t0/(epoch*m+i + t1)

                theta = theta - eta*gradients

                if i%50 == 0:
                    plt.figure(1)
                    self.show_res(theta)
                    plt.pause(0.001)

                    plt.figure("eta")
                    plt.scatter(epoch*m+i,eta)
                    plt.pause(0.0001)
        print(theta)

    def rand_grad_sklearn(self):
        """
        随机梯度下降 eta0:学习率 n_iter_no_change:迭代次数
        :return:
        """
        from sklearn.linear_model import SGDRegressor
        sgd_reg = SGDRegressor(n_iter_no_change=50,penalty=None,eta0=0.1)
        sgd_reg.fit(self.x,self.y.ravel())
        theta = [sgd_reg.intercept_, sgd_reg.coef_]
        print(theta)

    def ridge_regression(self,**option):
        """
        岭回归
        J(theta) = MSE(theta) + alpha/2 * theta.T.dot(theta)
        闭式解岭回归
        theta = np.linalg.inv((X.T.dot(X) + alpha*I)).dot((X.T).dot(Y)
        套索回归
        J(theta) = alpha(theta) + alpha* abs(theta)
        弹性网络成本函数
        J(theta) = alpha(theta) + r * alpha* abs(theta) + (1-r)/2 * alpha/2 * theta.T.dot(theta)


        """
        from sklearn.linear_model import Ridge
        ridge_reg = Ridge(alpha=1, solver="cholesky") # Cholesky 的矩阵因式分解法，闭式解
        ridge_reg.fit(self.x,self.y)
        theta = [ridge_reg.intercept_,ridge_reg.coef_]
        print("ridge_reg theta:", theta)

        """岭回归"""
        from sklearn.linear_model import SGDRegressor
        # 超参数 penalty设置正则项 "l2"权重向量I2范数的一半，岭回归
        sgd_reg = SGDRegressor(penalty="l2")
        sgd_reg.fit(self.x,self.y.ravel())
        theta = [sgd_reg.intercept_, sgd_reg.coef_]
        print("sgd_reg theta:", theta)

        """套索回归"""
        from sklearn.linear_model import SGDRegressor
        # 超参数 penalty设置正则项 "l1"权重向量I1范数，套索回归
        sgd_reg = SGDRegressor(penalty="l1")
        sgd_reg.fit(self.x, self.y.ravel())
        theta = [sgd_reg.intercept_, sgd_reg.coef_]
        print("sgd_reg I1 theta:", theta)

        from sklearn.linear_model import Lasso
        lasso_reg = Lasso(alpha=0.1)
        lasso_reg.fit(self.x,self.y)
        theta = [lasso_reg.intercept_, lasso_reg.coef_]
        print("lasso_reg theta:", theta)

        """弹性网络 """
        from sklearn.linear_model import ElasticNet
        elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
        elastic_net.fit(self.x,self.y)
        theta = [elastic_net.intercept_, elastic_net.coef_]
        print("elastic_net theta:", theta)

        """早期停止法基本实现"""
        from sklearn.metrics import mean_squared_error
        from sklearn import clone # 复制模型
        # warm_start = True 调用fit 方法后会从停下的地方继续开始训练，而不会重新开始
        # n_iter_no_change 迭代次数
        sgd_reg = SGDRegressor(n_iter_no_change=1, warm_start=True, penalty=None
                               , learning_rate="invscaling")
        minimun_val_error = float("+inf") # 正无穷
        best_epoch = None
        best_model = None
        for epoch in range(1000): # 叠代1000次，取其中最好的参数
            sgd_reg.fit(self.x, self.y.ravel())
            y_val_predict = sgd_reg.predict(self.x)
            val_error = mean_squared_error(y_val_predict,self.y)
            if val_error < minimun_val_error:
                minimun_val_error = val_error
                best_epoch = epoch
                best_model = clone(sgd_reg)
        print(best_model,"\n best_epoch: ", best_epoch,"\n theta:",[sgd_reg.intercept_,sgd_reg.coef_])

class poly_pred(object):
    """
    多项式回归
    PolynomialFeatures 可以将一个包含n个特征的数组转换为(n+d)/(n!d!)个特征的数组，当心数据爆炸！
    """
    def __init__(self):
        self.m = 100
        self.x = 6 * np.random.rand(self.m, 1) -3
        self.z = 10 * np.random.rand(self.m, 1)
        self.y = 0.5*self.x**2 + self.x  + 0*self.z + 2 + np.random.randn(self.m,1)
        plt.scatter(self.x, self.y)
        plt.xlabel('x'); plt.ylabel('y')

    def view(self,theta):

        x_b = np.concatenate((np.ones((self.m,1)),self.x_poly, self.z),axis=1)
        y_pred = x_b.dot(theta)
        plt.scatter(self.x, y_pred)
        # plt.pause(0.01)

    def poly_linear(self):
        from sklearn.preprocessing import PolynomialFeatures
        from sklearn.linear_model import LinearRegression
        poly_features = PolynomialFeatures(degree=2, include_bias=False)
        # 表达式中有x**2 可PolynomialFeatures获取该特征
        self.x_poly = poly_features.fit_transform(self.x)

        # 根据表达式进行特征重组, 各特征位置与预测的theta相关
        x_b = np.concatenate((self.x_poly, self.z), axis=1)
        lin_reg = LinearRegression()
        lin_reg.fit(x_b,self.y)
        theta = [lin_reg.intercept_, lin_reg.coef_]
        theta = np.array(theta[0].tolist() + theta[1].tolist()[0]).reshape(-1,1)
        # self.view(theta)
        return theta

def learn_curve(model):
    """分析不同数量训练集对训练结果的影响"""
    from sklearn.metrics import mean_squared_error
    from sklearn.model_selection import train_test_split

    x = 6 * np.random.rand(100, 1) - 3
    y = 0.5 * x ** 2 + x + 2 + np.random.randn(100, 1)

    def view(model=model):
        """观测不同数量数据集对模型训练结果影响"""
        x_train,x_val,y_train,y_val = train_test_split(x,y)
        train_error, val_errors = [],[]
        for m in range(1,len(x_train)):
            model.fit(x_train[:m],y_train[:m])
            y_train_predict = model.predict(x_train[:m])
            y_val_predict = model.predict(x_val)
            train_error.append(mean_squared_error(y_train_predict,y_train[:m]))
            val_errors.append(mean_squared_error(y_val,y_val_predict))
        plt.plot(np.sqrt(train_error),"r-+",linewidth=2,label="train")
        plt.plot(np.sqrt(val_errors),"b-",linewidth=3,label="val")
        plt.legend(loc="upper right")
        plt.axis([-0.5,80,-0.5,4])

    view()

class decision_edge(object):
    def __init__(self):
        from sklearn import datasets
        self.iris = datasets.load_iris()
        self.x = self.iris["data"][:,3:] # petal width
        self.y = (self.iris["target"]==2).astype(np.int) # 1 if Iris-Virginica, else 0

        from sklearn.linear_model import LogisticRegression
        log_reg = LogisticRegression()
        log_reg.fit(self.x,self.y)
        self.x_new = np.linspace(0, 3, 1000).reshape(-1, 1)
        y_proba = log_reg.predict_proba(self.x_new)
        plt.plot(self.x_new, y_proba[:,1], "g-", label="Iris-Virginica")
        plt.plot(self.x_new, y_proba[:,0], "b--", label="Not Iris-Virginica")
        plt.xlabel("花瓣宽度")
        plt.ylabel("概率")
        plt.legend()

temp = decision_edge()
print(temp.iris)





if __name__ == "__main__":

    case = [4]

    if 1 in case :
        tem = linera_pred()
        # tem.rand_grad(batch=20)
        tem.ridge_regression(alfa=0)
        tem.ridge_regression(alfa=1)
        tem.ridge_regression(alfa=10)
        tem.ridge_regression(alfa=100)

    elif 2 in case :
        tem = poly_pred()
        tem.view(tem.poly_linear())
    elif 3 in case :
        # plt.figure("linear_features")
        # lin_reg = LinearRegression()
        # learn_curve(lin_reg)
        plt.pause(0.01)
        plt.figure("poly_features")
        # 使用3级多项式进行拟合，看学习率随训练集变化
        polynominal_regression = Pipeline([
            ("poly_features",PolynomialFeatures(degree=3,include_bias=False)),
            ("sgd_reg",LinearRegression()),
        ])
        learn_curve(polynominal_regression)

    elif 4 in case:
        tem = linera_pred()
        tem.ridge_regression()










