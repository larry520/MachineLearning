#!usr/bin/env python
#-*- coding:utf-8 -*-
"""
@author:dmsoft
@file: paper_practice.py
@time: 2020/03/03
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

from sklearn.pipeline import Pipeline

import time

class linera_pred(object):
    """
    线性回归
    """

    def __init__(self):
        self.x = 2 * np.random.rand(100, 1)  # rand 随机(0~1) randn 正态分布随机数(-1~1)
        self.y = 4 + 3 * self.x + np.random.randn(100, 1)  # 4*x0 + 3*x1 +random
        self.x_b = np.concatenate((np.ones((100, 1)), self.x), axis=1)  # add x0=1 to each instance

    def show_res(self,theta):
        x_new = np.array([[0], [2]])
        x_new_b = np.c_[np.ones((2, 1)), x_new]
        y_predict = x_new_b.dot(theta)
        plt.plot(x_new, y_predict, 'r-')
        plt.axis([0, 2, 0, 15])
        plt.legend(["predict", "real"], loc="upper left")
        plt.show()

    def linear_reg(self):
        """线性回归，直接公式法  theata = (xT.x)-1 . xT . y """
        theta_best = np.linalg.inv(self.x_b.T.dot(self.x_b)).dot(self.x_b.T).dot(self.y)
        plt.scatter(self.x, self.y)
        self.show_res(theta_best)

    def linear_reg_sklearn(self):
        """采用sklearn  LinearRegression模型，直接求特征值法
        偏置项:lin_reg.intercept_ , 特征权重：lin_reg.coef_"""
        from sklearn.linear_model import LinearRegression
        lin_reg = LinearRegression()
        lin_reg.fit(self.x,self.y)
        theta = [lin_reg.intercept_, lin_reg.coef_]
        plt.scatter(self.x, self.y)
        self.show_res(theta)

    def lin_grad(self):
        """批量梯度下降法 gradients = 2/m * X_b.T.dot(X_b
        批量梯度下降 超参数：学习率、迭代次数
        学习率：步长 过高->发散  过低->耗时
        对局部最小值曲线难以得到全局最小值"""
        eta = 0.1 # learning rate
        n_iterations = 1000
        m = 100
        theta = np.random.randn(2,1)
        plt.scatter(self.x, self.y)
        for iteration in range(n_iterations):
            gradients = 2/m * self.x_b.T.dot(self.x_b.dot(theta) -self.y)
            theta = theta - eta*gradients
            MSE = 0
            for i in range(self.y.shape[0]):
                MSE += 1/m *(((theta.T.dot(self.x_b[i]) - self.y[i])[0])**2)

            if iteration%20 == 0:
                print(MSE,theta)
                self.show_res(theta)
                plt.pause(0.01)

    def rand_grad(self,batch=1):
        """
         随机梯度下降法 超参数:迭代次数、学习率（t0,t1)
         gradients = 2*xi.T.dot(xi.dot(theta)-yi)
         advantage: 有助于跳出局部最小值
         shortage: 永远找不出最小值，可逐步降低学习率使结果尽量靠近
        """
        n_epochs = 50
        t0,t1 = 5,50 # learning schedule hyper-parameter
        m = 100
        theta = np.random.randn(2,1) # random initialization

        plt.scatter(self.x,self.y)
        for epoch in range(n_epochs):
            for i in range(m):
                random_index = np.random.randint(m)
                # 随机梯度下降
                if batch == 1 or not isinstance(batch,int):
                    xi = self.x_b[random_index:random_index+1]  # xi.shape: [1,2]
                    yi = self.y[random_index:random_index+1]
                    gradients = 2 * xi.T.dot(xi.dot(theta) - yi)

                # 小批量梯度随机下降
                else :
                    xi = self.x_b[random_index:random_index + batch]  # xi.shape: [1,2]
                    yi = self.y[random_index:random_index + batch]
                    gradients = 2/batch*xi.T.dot(xi.dot(theta)-yi)

                eta = t0/(epoch*m+i + t1)

                theta = theta - eta*gradients

                if i%50 == 0:
                    plt.figure(1)
                    self.show_res(theta)
                    plt.pause(0.001)

                    plt.figure("eta")
                    plt.scatter(epoch*m+i,eta)
                    plt.pause(0.0001)
        print(theta)

    def rand_grad_sklearn(self):
        """
        随机梯度下降 eta0:学习率 n_iter_no_change:迭代次数
        :return:
        """
        from sklearn.linear_model import SGDRegressor
        sgd_reg = SGDRegressor(n_iter_no_change=50,penalty=None,eta0=0.1)
        sgd_reg.fit(self.x,self.y.ravel())
        theta = [sgd_reg.intercept_, sgd_reg.coef_]
        print(theta)

class poly_pred(object):
    """
    多项式回归
    PolynomialFeatures 可以将一个包含n个特征的数组转换为(n+d)/(n!d!)个特征的数组，当心数据爆炸！
    """
    def __init__(self):
        self.m = 100
        self.x = 6 * np.random.rand(self.m, 1) -3
        self.z = 10 * np.random.rand(self.m, 1)
        self.y = 0.5*self.x**2 + self.x  + 0*self.z + 2 + np.random.randn(self.m,1)
        plt.scatter(self.x, self.y)
        plt.xlabel('x'); plt.ylabel('y')

    def view(self,theta):

        x_b = np.concatenate((np.ones((self.m,1)),self.x_poly, self.z),axis=1)
        y_pred = x_b.dot(theta)
        plt.scatter(self.x, y_pred)
        # plt.pause(0.01)

    def poly_linear(self):
        from sklearn.preprocessing import PolynomialFeatures
        from sklearn.linear_model import LinearRegression
        poly_features = PolynomialFeatures(degree=2, include_bias=False)
        # 表达式中有x**2 可PolynomialFeatures获取该特征
        self.x_poly = poly_features.fit_transform(self.x)

        # 根据表达式进行特征重组, 各特征位置与预测的theta相关
        x_b = np.concatenate((self.x_poly, self.z), axis=1)
        lin_reg = LinearRegression()
        lin_reg.fit(x_b,self.y)
        theta = [lin_reg.intercept_, lin_reg.coef_]
        theta = np.array(theta[0].tolist() + theta[1].tolist()[0]).reshape(-1,1)
        # self.view(theta)
        return theta


def learn_curve(model):
    """分析不同数量训练集对训练结果的影响"""
    from sklearn.metrics import mean_squared_error
    from sklearn.model_selection import train_test_split

    x = 6 * np.random.rand(100, 1) - 3
    y = 0.5 * x ** 2 + x + 2 + np.random.randn(100, 1)

    def view(model=model):
        x_train,x_val,y_train,y_val = train_test_split(x,y)
        train_error, val_errors = [],[]
        for m in range(1,len(x_train)):
            model.fit(x_train[:m],y_train[:m])
            y_train_predict = model.predict(x_train[:m])
            y_val_predict = model.predict(x_val)
            train_error.append(mean_squared_error(y_train_predict,y_train[:m]))
            val_errors.append(mean_squared_error(y_val,y_val_predict))
        plt.plot(np.sqrt(train_error),"r-+",linewidth=2,label="train")
        plt.plot(np.sqrt(val_errors),"b-",linewidth=3,label="val")
        plt.axis([-0.5,80,-0.5,4])


    view()








if __name__ == "__main__":

    case = [3]

    if 1 in case :
        tem = linera_pred()
        tem.rand_grad(batch=20)
    elif 2 in case :
        tem = poly_pred()
        tem.view(tem.poly_linear())
    elif 3 in case :
        # plt.figure("linear_features")
        # lin_reg = LinearRegression()
        # learn_curve(lin_reg)
        plt.pause(0.01)
        plt.figure("poly_features")
        polynominal_regression = Pipeline([
            ("poly_features",PolynomialFeatures(degree=10,include_bias=False)),
            ("sgd_reg",LinearRegression()),
        ])
        learn_curve(polynominal_regression)









